---
layout: post
title:  "Scaling Pre-trained Language Models to Deeper via Parameter-efficient Architecture"
date:   2023-07-16 18:04:44 +00:00
image: images/emergency.png
categories: 2023_research
author: Ze-Feng Gao
authors: "Peiyu Liu, Zikang Liu, <strong>Ze-Feng Gao</strong>, Dawei Gao, Wayne Xin Zhao,Yaliang Li, Bolin Ding, Ji-Rong Wen"
venue: "Arxiv"
paper: /pdfs/emergency.pdf
arxiv: https://arxiv.org/abs/2307.08072

---
This work aims to investigate the impact of quantization on \emph{emergent abilities}, which are important characteristics that distinguish LLMs from small language models. Specially, we examine the abilities of in-context learning, chain-of-thought reasoning, and instruction-following in quantized LLMs. 