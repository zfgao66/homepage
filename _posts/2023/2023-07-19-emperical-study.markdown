---
layout: post
title:  "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study"
date:   2023-07-16 18:04:44 +00:00
image: images/2024/emergency.png
categories: 2024_research
author: Ze-Feng Gao
authors: "Peiyu Liu, Zikang Liu, <strong>Ze-Feng Gao</strong>, Dawei Gao, Wayne Xin Zhao,Yaliang Li, Bolin Ding, Ji-Rong Wen"
venue: "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (COLING 2024)"
paper: /pdfs/2024/emergency.pdf
arxiv: https://arxiv.org/abs/2307.08072
link: https://aclanthology.org/2024.lrec-main.461
code: https://github.com/RUCAIBox/QuantizedEmpirical


---
This work aims to investigate the impact of quantization on \emph{emergent abilities}, which are important characteristics that distinguish LLMs from small language models. Specially, we examine the abilities of in-context learning, chain-of-thought reasoning, and instruction-following in quantized LLMs. 

