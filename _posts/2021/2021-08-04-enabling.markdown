---
layout: post
title:  "Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators"
date:   2021-08-04 18:04:44 +00:00
image: images/2021/enabling_pic.png
categories: 2021_research
author: Ze-Feng Gao
authors: "Peiyu Liu*, <strong>Ze-Feng Gao*</strong>, Wayne Xin Zhao<sup>#</sup>, Z.Y. Xie, Zhong-Yi Lu<sup>#</sup>, Ji-Rong Wen"
venue: "Annual Meeting of the Association for Computational Linguistics (ACL2021), Poster"
paper: /pdfs/2021/acl2021.pdf
arxiv: https://arxiv.org/abs/2106.02205
slides: /pdfs/acl_liupeiyu_4113.pdf
code: https://github.com/RUCAIBox/MPOP
link: https://aclanthology.org/2021.acl-long.418/
---
This paper presents a novel pre-trained language models (PLM) compression approach based on the matrix product operator (short as MPO) from quantum many-body physics.