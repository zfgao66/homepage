---
layout: post
title:  "Scaling Pre-trained Language Models to Deeper via Parameter-efficient Architecture"
date:   2023-05-07 18:04:44 +00:00
image: images/mpobert.png
categories: 2023_research
author: Ze-Feng Gao
authors: "Peiyu Liu*, <strong>Ze-Feng Gao*</strong>, Wayne Xin Zhao, Ji-Rong Wen"
venue: "Arxiv"
paper: /pdfs/mpobert.pdf
arxiv: https://arxiv.org/abs/2303.16753
code: https://github.com/zfgao66/OPF

---
In this paper, we propose a parameter-efficient pre-training approach that utilizes matrix decomposition and parameter-sharing strategies to scale PLMs. Extensive experiments have demonstrated the effectiveness of our proposed model in reducing the model size and achieving highly competitive performance (i.e. with fewer parameters than BERT-base, we successfully scale the model depth by a factor of 4x and even achieve 0.1 points higher than BERT-large for GLUE score).